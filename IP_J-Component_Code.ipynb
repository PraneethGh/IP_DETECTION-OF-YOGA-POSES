{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ipjcompipynb-r3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "DETECTION OF YOGA POSES USING OPENPOSE AND CNN"
      ],
      "metadata": {
        "id": "tuc9VOVr5sYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Necessary Libraries\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import random\n",
        "import cv2\n",
        "import re\n",
        "import numpy as np\n",
        "#!pip install keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image, ImageFilter"
      ],
      "metadata": {
        "id": "20maOQbefnll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Dataset from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "odIs4MQcPM-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Snippet of code for visualizing effect of filters\n",
        "path='/content/drive/MyDrive/yoga_dataset/'\n",
        "for i in os.scandir(path):\n",
        "  ct = 0\n",
        "  pose = os.path.basename(i)\n",
        "  path2 = os.path.join(path,pose)\n",
        "  l =[]\n",
        "  for j in os.listdir(path2):\n",
        "    if(ct==3):\n",
        "      break\n",
        "    path3 = os.path.join(path2,j)\n",
        "    img = cv2.imread(path3)[...,::-1]\n",
        "    blur = cv2.GaussianBlur(img,(5,5),0)\n",
        "    plt.subplot(151),plt.imshow(img),plt.title('Original')\n",
        "    plt.xticks([]), plt.yticks([])\n",
        "    plt.subplot(152),plt.imshow(blur),plt.title('Gaussian')\n",
        "    plt.xticks([]), plt.yticks([])\n",
        "    img_median = cv2.medianBlur(blur,5) \n",
        "    plt.subplot(153),plt.imshow(img_median),plt.title('Median filter')\n",
        "    plt.xticks([]), plt.yticks([])\n",
        "    img2 = Image.fromarray(img_median, 'RGB')\n",
        "    im2 = img2.filter(ImageFilter.UnsharpMask(percent=150))\n",
        "    plt.subplot(154),plt.imshow(im2),plt.title('Sharpened')\n",
        "    plt.xticks([]), plt.yticks([])\n",
        "    im3 = im2.filter(ImageFilter.EDGE_ENHANCE_MORE)\n",
        "    plt.subplot(155),plt.imshow(im2),plt.title('Edge-enhanced')\n",
        "    plt.xticks([]), plt.yticks([])\n",
        "    plt.show()\n",
        "    ct=ct+1"
      ],
      "metadata": {
        "id": "pDv2rq2p2JQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without **OpenPose** and **Filters**"
      ],
      "metadata": {
        "id": "IVm5q5BSG1z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to read the images without passing through any filters\n",
        "labels = []\n",
        "images = []\n",
        "asanas_name = []\n",
        "images_path = []\n",
        "images_pixels = []\n",
        "mapping={}\n",
        "i=0\n",
        "dataset_path = '/content/drive/MyDrive/yoga_dataset/'\n",
        "\n",
        "for directory in os.listdir(dataset_path):\n",
        "    asanas_name.append(directory)\n",
        "    print(directory)\n",
        "    c=0\n",
        "    for img in os.listdir(os.path.join(dataset_path,directory)):  \n",
        "        if len(re.findall('.png',img.lower())) != 0 or len(re.findall('.jpg',img.lower())) != 0 or len(re.findall('.jpeg',img.lower())) != 0:\n",
        "            img_path = os.path.join(os.path.join(dataset_path,directory),img)\n",
        "            images.append(img)\n",
        "            images_path.append(img_path)\n",
        "            #print(img)\n",
        "            img_pix = cv2.imread(img_path,1)\n",
        "            images_pixels.append(cv2.resize(img_pix, (100,100)))\n",
        "            labels.append(i)\n",
        "            c+=1\n",
        "    mapping[i]=directory\n",
        "    i = i+1\n",
        "    if i==20:\n",
        "      break\n",
        "\n",
        "    \n",
        "print(\"Total labels: \", len(labels))\n",
        "print(\"Total images: \", len(images))\n",
        "print(\"Total images path: \", len(images_path))\n",
        "print(\"Total asanas: \", len(asanas_name))\n",
        "print(\"Total images_pixels: \", len(images_pixels))"
      ],
      "metadata": {
        "id": "ZNh51tySopOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(images_pixels[0])"
      ],
      "metadata": {
        "id": "9zQyaUtwnnjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/misbah4064/human-pose-estimation-opencv.git\n",
        "%cd human-pose-estimation-opencv/"
      ],
      "metadata": {
        "id": "-EPvAg0TMDgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenPose Code\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "BODY_PARTS = { \"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
        "               \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
        "               \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n",
        "               \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18 }\n",
        "\n",
        "POSE_PAIRS = [ [\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
        "               [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"],\n",
        "               [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"], [\"Neck\", \"LHip\"],\n",
        "               [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"], [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"],\n",
        "               [\"REye\", \"REar\"], [\"Nose\", \"LEye\"], [\"LEye\", \"LEar\"] ]\n",
        "\n",
        "width = 368\n",
        "height = 368\n",
        "inWidth = width\n",
        "inHeight = height\n",
        "\n",
        "net = cv.dnn.readNetFromTensorflow(\"graph_opt.pb\")\n",
        "thr = 0.2\n",
        "\n",
        "# Defining function which uses OpenPose\n",
        "def poseDetector(frame):\n",
        "    frameWidth = frame.shape[1]\n",
        "    frameHeight = frame.shape[0]\n",
        "    \n",
        "    net.setInput(cv.dnn.blobFromImage(frame, 1.0, (inWidth, inHeight), (127.5, 127.5, 127.5), swapRB=True, crop=False))\n",
        "    out = net.forward()\n",
        "    out = out[:, :19, :, :]  # MobileNet output [1, 57, -1, -1], we only need the first 19 elements\n",
        "\n",
        "    assert(len(BODY_PARTS) == out.shape[1])\n",
        "\n",
        "    points = []\n",
        "    for i in range(len(BODY_PARTS)):\n",
        "        # Slice heatmap of corresponging body's part.\n",
        "        heatMap = out[0, i, :, :]\n",
        "\n",
        "        _, conf, _, point = cv.minMaxLoc(heatMap)\n",
        "        x = (frameWidth * point[0]) / out.shape[3]\n",
        "        y = (frameHeight * point[1]) / out.shape[2]\n",
        "        points.append((int(x), int(y)) if conf > thr else None)\n",
        "\n",
        "    for pair in POSE_PAIRS:\n",
        "        partFrom = pair[0]\n",
        "        partTo = pair[1]\n",
        "        assert(partFrom in BODY_PARTS)\n",
        "        assert(partTo in BODY_PARTS)\n",
        "\n",
        "        idFrom = BODY_PARTS[partFrom]\n",
        "        idTo = BODY_PARTS[partTo]\n",
        "\n",
        "        if points[idFrom] and points[idTo]:\n",
        "            cv.line(frame, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
        "            cv.ellipse(frame, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
        "            cv.ellipse(frame, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
        "\n",
        "    t, _ = net.getPerfProfile()\n",
        "\n",
        "    return frame"
      ],
      "metadata": {
        "id": "V8Zb-rsgMLLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function related OpenPose\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(16, 16)\n",
        "\n",
        "next_pix = images_path\n",
        "# fimg and flabel will contain the images with OpenPose marking and corresponding labels\n",
        "fimg=[]\n",
        "flabel=[]\n",
        "for i, img_path in enumerate(next_pix):\n",
        "    img = cv.imread(img_path)\n",
        "    #print(type(img))\n",
        "    output = poseDetector(img)\n",
        "    #cv2_imshow(output)\n",
        "    fimg.append(cv2.resize(output, (100,100)))\n",
        "    flabel.append(labels[i])\n",
        "\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "8YPJQzI7MOiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(fimg[0])"
      ],
      "metadata": {
        "id": "1neKPDmTyYK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = cv.imread(\"image.jpg\")\n",
        "output = poseDetector(input)\n",
        "#cv2_imshow(output)"
      ],
      "metadata": {
        "id": "emK1jMk9MnYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "dgen = list(zip(images_pixels,labels))\n",
        "random.shuffle(dgen)\n",
        "\n",
        "trset,lab = zip(*dgen)\n",
        "\n"
      ],
      "metadata": {
        "id": "wvGAH4CejrsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels[0]"
      ],
      "metadata": {
        "id": "a200gNlH6q6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array(trset) / 255\n",
        "y =  to_categorical(lab, num_classes = 20)\n",
        "# Splitting the records into Training and Testing Set\n",
        "xtrain,xtest,ytrain,ytest = train_test_split(x,y, test_size = 0.2, random_state=101)\n",
        "\n",
        "print(\"Number of training set records \", len(xtrain))\n",
        "print(\"Number of testing set records : \", len(xtest))"
      ],
      "metadata": {
        "id": "iyS0wD0uaquD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgdata = ImageDataGenerator(horizontal_flip=False,vertical_flip=False,rotation_range=0,\n",
        "                             zoom_range=0.2,width_shift_range=0,height_shift_range=0,shear_range=0,fill_mode=\"nearest\")"
      ],
      "metadata": {
        "id": "a4lWDtp0kIKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Pre trained DenseNet121 CNN Model\n",
        "densemod = tf.keras.applications.DenseNet121(input_shape=(100,100,3),include_top=False,weights='imagenet',pooling='avg')\n",
        "\n",
        "densemod.trainable = False"
      ],
      "metadata": {
        "id": "Eylq81yVXDwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding input,output and drop layers\n",
        "ip = densemod.input\n",
        "op=densemod.output\n",
        "\n",
        "dropout_1 = tf.keras.layers.Dropout(0.25)(op)\n",
        "layer_x_1 = tf.keras.layers.Dense(512, activation='relu')(dropout_1)\n",
        "layer_x_2 = tf.keras.layers.Dense(128, activation='relu')(layer_x_1)\n",
        "dropout_2 = tf.keras.layers.Dropout(0.20)(layer_x_2)\n",
        "res = tf.keras.layers.Dense(20, activation='softmax')(dropout_2)\n",
        "\n",
        "\n",
        "train_mod = tf.keras.Model(inputs=ip, outputs=res)"
      ],
      "metadata": {
        "id": "VZEXIvdJXKQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model for 20 epochs\n",
        "adam_res = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "train_mod.compile(optimizer=adam_res,loss='categorical_crossentropy',metrics=['acc'])\n",
        "fresult = train_mod.fit(imgdata.flow(xtrain,ytrain,batch_size=32),validation_data=(xtest,ytest),epochs=20)"
      ],
      "metadata": {
        "id": "ZxwmZRY_XPMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the accuracy obtained across each epoch\n",
        "trainacc = fresult.history['acc']\n",
        "testacc = fresult.history['val_acc']\n",
        "trainloss = fresult.history['loss']\n",
        "testloss = fresult.history['val_loss']\n",
        "ep = range(len(trainacc))\n",
        "\n",
        "plt.plot(ep, trainacc, 'r', label='Training accuracy')\n",
        "plt.plot(ep, testacc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jSXe1CcIXTJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred=train_mod.predict(xtrain)"
      ],
      "metadata": {
        "id": "JZnH1mH3ytWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With **Openpose** but without **filters**"
      ],
      "metadata": {
        "id": "G1RIKTPA-qRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to train the model with original images passed through OpenPose library but not through filters\n",
        "import matplotlib.pyplot as plt\n",
        "dgen = list(zip(fimg,flabel))\n",
        "random.shuffle(dgen)\n",
        "\n",
        "tr_set,lab = zip(*dgen)\n",
        "\n",
        "x = np.array(tr_set) / 255\n",
        "y =  to_categorical(lab, num_classes = 20)\n",
        "\n",
        "xtrain,xtest,ytrain,ytest = train_test_split(x,y, test_size = 0.2, random_state=101)\n",
        "\n",
        "print(\"Number of training set records \", len(xtrain))\n",
        "print(\"Number of testing set records : \", len(xtest))\n",
        "\n",
        "imgdata = ImageDataGenerator(horizontal_flip=False,vertical_flip=False,rotation_range=0,\n",
        "                             zoom_range=0.2,width_shift_range=0,height_shift_range=0,shear_range=0,fill_mode=\"nearest\")\n",
        "\n",
        "densemod = tf.keras.applications.DenseNet121(input_shape=(100,100,3),include_top=False,weights='imagenet',pooling='avg')\n",
        "\n",
        "densemod.trainable = False\n",
        "ip = densemod.input\n",
        "op=densemod.output\n",
        "\n",
        "dropout_1 = tf.keras.layers.Dropout(0.25)(op)\n",
        "layer_x_1 = tf.keras.layers.Dense(512, activation='relu')(dropout_1)\n",
        "layer_x_2 = tf.keras.layers.Dense(128, activation='relu')(layer_x_1)\n",
        "dropout_2 = tf.keras.layers.Dropout(0.20)(layer_x_2)\n",
        "res = tf.keras.layers.Dense(20, activation='softmax')(dropout_2)\n",
        "\n",
        "\n",
        "train_mod = tf.keras.Model(inputs=ip, outputs=res)\n",
        "\n",
        "adam_res = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "train_mod.compile(optimizer=adam_res,loss='categorical_crossentropy',metrics=['acc'])\n",
        "fresult = train_mod.fit(imgdata.flow(xtrain,ytrain,batch_size=32),validation_data=(xtest,ytest),epochs=20)\n",
        "\n",
        "trainacc = fresult.history['acc']\n",
        "trainloss = fresult.history['loss']\n",
        "testacc = fresult.history['val_acc']\n",
        "testloss = fresult.history['val_loss']\n",
        "ep = range(len(trainacc))\n",
        "\n",
        "plt.plot(ep, trainacc, 'r', label='Training accuracy')\n",
        "plt.plot(ep, testacc, 'b', label='Testing accuracy')\n",
        "plt.title('Accuracy Analysis : training vs testing')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()\n",
        "print(\"Max accuracy:\"+str(max(testacc)))"
      ],
      "metadata": {
        "id": "GabuM9VX-0DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without **OpenPose** but with **Filters**"
      ],
      "metadata": {
        "id": "mISOr_TeEAzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to train the model with images passed through filters but not through OpenPose\n",
        "labels = []\n",
        "images = []\n",
        "asanas_name = []\n",
        "images_path = []\n",
        "images_pixels = []\n",
        "\n",
        "i=0\n",
        "dataset_path = '/content/drive/MyDrive/yoga_dataset/'\n",
        "\n",
        "for directory in os.listdir(dataset_path):\n",
        "    asanas_name.append(directory)\n",
        "    print(directory)\n",
        "    c=0\n",
        "    for img in os.listdir(os.path.join(dataset_path,directory)):  \n",
        "        if len(re.findall('.png',img.lower())) != 0 or len(re.findall('.jpg',img.lower())) != 0 or len(re.findall('.jpeg',img.lower())) != 0:\n",
        "            img_path = os.path.join(os.path.join(dataset_path,directory),img)\n",
        "            images.append(img)\n",
        "            images_path.append(img_path)\n",
        "            #print(img)\n",
        "            img_pix = cv2.imread(img_path,1)\n",
        "            # Preprocessing\n",
        "            blur = cv2.GaussianBlur(img_pix,(5,5),0)\n",
        "            img_m = cv2.medianBlur(blur,5)\n",
        "            img2 = Image.fromarray(img_m, 'RGB')\n",
        "            im2 = img2.filter(ImageFilter.UnsharpMask(percent=150))\n",
        "            im3 = im2.filter(ImageFilter.EDGE_ENHANCE_MORE)\n",
        "            #appending\n",
        "            images_pixels.append(cv2.resize(img_m, (100,100)))\n",
        "            labels.append(i)\n",
        "            c+=1\n",
        "    i = i+1\n",
        "    if i==20:\n",
        "      break    \n",
        "print(\"Total labels: \", len(labels))\n",
        "print(\"Total images: \", len(images))\n",
        "print(\"Total images path: \", len(images_path))\n",
        "print(\"Total asanas: \", len(asanas_name))\n",
        "print(\"Total images_pixels: \", len(images_pixels))"
      ],
      "metadata": {
        "id": "hplXYStQADf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "dgen = list(zip(images_pixels,labels))\n",
        "random.shuffle(dgen)\n",
        "\n",
        "tr_set,lab = zip(*dgen)\n",
        "\n",
        "x = np.array(tr_set) / 255\n",
        "y =  to_categorical(lab, num_classes = 20)\n",
        "\n",
        "xtrain,xtest,ytrain,ytest = train_test_split(x,y, test_size = 0.2, random_state=101)\n",
        "\n",
        "print(\"Number of training set records \", len(xtrain))\n",
        "print(\"Number of testing set records : \", len(xtest))\n",
        "\n",
        "imgdata = ImageDataGenerator(horizontal_flip=False,vertical_flip=False,rotation_range=0,\n",
        "                             zoom_range=0.2,width_shift_range=0,height_shift_range=0,shear_range=0,fill_mode=\"nearest\")\n",
        "\n",
        "densemod = tf.keras.applications.DenseNet121(input_shape=(100,100,3),include_top=False,weights='imagenet',pooling='avg')\n",
        "\n",
        "densemod.trainable = False\n",
        "ip = densemod.input\n",
        "op=densemod.output\n",
        "\n",
        "dropout_1 = tf.keras.layers.Dropout(0.25)(op)\n",
        "layer_x_1 = tf.keras.layers.Dense(512, activation='relu')(dropout_1)\n",
        "layer_x_2 = tf.keras.layers.Dense(128, activation='relu')(layer_x_1)\n",
        "dropout_2 = tf.keras.layers.Dropout(0.20)(layer_x_2)\n",
        "res = tf.keras.layers.Dense(20, activation='softmax')(dropout_2)\n",
        "\n",
        "\n",
        "train_mod = tf.keras.Model(inputs=ip, outputs=res)\n",
        "\n",
        "adam_res = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "train_mod.compile(optimizer=adam_res,loss='categorical_crossentropy',metrics=['acc'])\n",
        "fresult = train_mod.fit(imgdata.flow(xtrain,ytrain,batch_size=32),validation_data=(xtest,ytest),epochs=20)\n",
        "\n",
        "trainacc = fresult.history['acc']\n",
        "trainloss = fresult.history['loss']\n",
        "testacc = fresult.history['val_acc']\n",
        "testloss = fresult.history['val_loss']\n",
        "ep = range(len(trainacc))\n",
        "\n",
        "plt.plot(ep, trainacc, 'r', label='Training accuracy')\n",
        "plt.plot(ep, testacc, 'b', label='Testing accuracy')\n",
        "plt.title('Accuracy Analysis : training vs testing')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()\n",
        "print(\"Max accuracy:\"+str(max(testacc)))"
      ],
      "metadata": {
        "id": "ilBCS9UREMrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With **OpenPose and filters**"
      ],
      "metadata": {
        "id": "d6eyz0Ef8WOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Default title text\n",
        "# Code to train the model with original images passed through OpenPose and through filters\n",
        "import matplotlib.pyplot as plt\n",
        "dgen = list(zip(fimg,flabel))\n",
        "random.shuffle(dgen)\n",
        "\n",
        "tr_set,lab = zip(*dgen)\n",
        "\n",
        "x = np.array(tr_set) / 255\n",
        "y =  to_categorical(lab, num_classes = 20)\n",
        "\n",
        "xtrain,xtest,ytrain,ytest = train_test_split(x,y, test_size = 0.2, random_state=101)\n",
        "\n",
        "print(\"Number of training set records \", len(xtrain))\n",
        "print(\"Number of testing set records : \", len(xtest))\n",
        "\n",
        "imgdata = ImageDataGenerator(horizontal_flip=False,vertical_flip=False,rotation_range=0,\n",
        "                             zoom_range=0.2,width_shift_range=0,height_shift_range=0,shear_range=0,fill_mode=\"nearest\")\n",
        "\n",
        "densemod = tf.keras.applications.DenseNet121(input_shape=(100,100,3),include_top=False,weights='imagenet',pooling='avg')\n",
        "\n",
        "densemod.trainable = False\n",
        "ip = densemod.input\n",
        "op=densemod.output\n",
        "\n",
        "dropout_1 = tf.keras.layers.Dropout(0.25)(op)\n",
        "layer_x_1 = tf.keras.layers.Dense(512, activation='relu')(dropout_1)\n",
        "layer_x_2 = tf.keras.layers.Dense(128, activation='relu')(layer_x_1)\n",
        "dropout_2 = tf.keras.layers.Dropout(0.20)(layer_x_2)\n",
        "res = tf.keras.layers.Dense(20, activation='softmax')(dropout_2)\n",
        "\n",
        "\n",
        "train_mod = tf.keras.Model(inputs=ip, outputs=res)\n",
        "\n",
        "adam_res = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "train_mod.compile(optimizer=adam_res,loss='categorical_crossentropy',metrics=['acc'])\n",
        "fresult = train_mod.fit(imgdata.flow(xtrain,ytrain,batch_size=32),validation_data=(xtest,ytest),epochs=20)\n",
        "\n",
        "trainacc = fresult.history['acc']\n",
        "trainloss = fresult.history['loss']\n",
        "testacc = fresult.history['val_acc']\n",
        "testloss = fresult.history['val_loss']\n",
        "ep = range(len(trainacc))\n",
        "\n",
        "plt.plot(ep, trainacc, 'r', label='Training accuracy')\n",
        "plt.plot(ep, testacc, 'b', label='Testing accuracy')\n",
        "plt.title('Accuracy Analysis : training vs testing')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()\n",
        "print(\"Max accuracy:\"+str(max(testacc)))"
      ],
      "metadata": {
        "id": "FTQeDUTB8alB",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred=train_mod.predict(xtrain)"
      ],
      "metadata": {
        "id": "R5tEcWbdvxSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(xtrain[0]*255)"
      ],
      "metadata": {
        "id": "9-lYWPWKAT0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lab=list(pred[0]).index(max(pred[0]))\n",
        "print(lab)\n",
        "print(mapping[lab])"
      ],
      "metadata": {
        "id": "r77BJZLPAPVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "acc=[82.683,84.84,78.35,81.385]\n",
        "lab=['Without OpenPose and filters','With OpenPose and Without Filters',' Without OpenPose and with filters','With OpenPose and Filters']\n",
        "plt.figure(figsize=(22,12))\n",
        "clrs = ['orange' if (x < max(acc)) else 'red' for x in acc]\n",
        "plt.bar(range(len(acc)),acc, align='center',color=clrs,edgecolor=\"yellow\",linewidth=5)\n",
        "plt.xticks(range(len(acc)), lab,fontweight='bold', fontsize='15', horizontalalignment='center')\n",
        "plt.ylabel(\"Accuracy\",fontweight='bold', fontsize='15', horizontalalignment='center')\n",
        "plt.yticks(fontweight='bold', fontsize='15')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jIu2JvFUbE0k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}